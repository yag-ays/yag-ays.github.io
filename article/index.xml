<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Article on Out-of-the-box</title>
    <link>https://yag-ays.github.io/article/</link>
    <description>Recent content in Article on Out-of-the-box</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja</language>
    <lastBuildDate>Sat, 16 Oct 2021 11:13:49 +0900</lastBuildDate><atom:link href="https://yag-ays.github.io/article/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>PyCon JP 2021にて「Pythonで始めるドキュメント・インテリジェンス入門」で登壇しました</title>
      <link>https://yag-ays.github.io/article/pyconjp2021/</link>
      <pubDate>Sat, 16 Oct 2021 11:13:49 +0900</pubDate>
      
      <guid>https://yag-ays.github.io/article/pyconjp2021/</guid>
      <description>2021/10/15-16に開催されたPyCon JP 2021にて、「Pythonで始めるドキュメント・インテリジェンス入門」という題で登壇しました。
ドキュメント・インテリジェンスとは、ビジネス書類を解析し内容を理解する技術やアプリケーションの総称です。紙に印字された文字をOCRで認識し、自然言語処理による文書解析や意味理解などの技術を組み合わせて、タスクに応じて解析システムを構築します。今回の私の発表では、ドキュメント・インテリジェンスの全体観を紹介するとともに、レシート読み取り機能の実装というシンプルな具体事例を元にして、OCRや情報抽出のイメージを掴んでもらうような内容にしました。
 ソースコード: https://github.com/yagays/di-pyconjp2021
具体の内容は発表資料を参照いただくとして、この記事ではどのような経緯で登壇に至ったかを振り返ってみようと思います。
 CfPへの応募 私が所属しているUbie Discoveryでは、ブログ執筆やカンファレンス登壇といった対外的なプレゼンス向上施策に力を入れています。そういった取り組みの中でPyCon JPのCall for Proposal（発表募集）が2021年6月頃に公開され、社内でも何かアクションを取れないかという話になりました。私も何か発表できるものがないかといろいろ検討した結果、Ubie入社から1年ほど仕事で取り組んでいたトピックである、紙に書かれた文書の解析について応募してみようと考えました。
ドキュメント・インテリジェンスという言葉 ドキュメント・インテリジェンスという言葉は、 Proposalの投稿に際して何か発表内容を一言で表す端的かつキャッチーな言葉が必要だと考え、NeurIPS 2019の&amp;ldquo;First Workshop on Workshop on Document Intelligence&amp;rdquo;というワークショップの名前から持ってきたものです。このワークショップは2021年のKDDで2回目が開かれるほどまだ歴史が浅く、タイトルをGoogleで検索しても先進的なスタートアップが自社商品の説明に利用する例が散見されるくらいで、おそらくdocument intelligence自体アメリカでもまだ広く浸透していない単語ではないかと思います。当然ながら日本語でドキュメント インテリジェンスと検索しても、本家document intelligenceと同じ概念として説明されたウェブページなどは出てきません。こうした言葉を使うことは、新しい単語を無闇矢鱈に発明せず先例に倣うというアカデミックの流儀からは外れるものではありましたが、色々とサーベイする中で適する言葉が見つからなかったこともあり、最終的に採用するに至りました。
ちなみに、概念としてはDocument Analysis and Recognitionが最も近いと思いますが、ドキュメント・インテリジェンスを含むもう少し大きな概念かと思います。ビジネス文書の解析はコンピュータを使ったタイポグラフィにより生成された書類を対象とし内容理解に重点を置くあたり、情景画像からのOCRであったり、手書き文書や歴史文書の解析、小説などの文字情報が全てを表す文書の解析とは、用いる手法やタスク設計が少し異なる印象があります。
投稿そして採択 そんなことを考えながらProposalを締め切りギリギリで書き上げて7月中旬に投稿し、結果が出る8月中旬まで待つことに。その当時slackでは以下のような呑気なことを書いており、こういった目論見は途方もなく間違っていたことをこの後実感します。
登壇に向けて 採択結果が8月10日にわかると、これはいよいよ頑張らないといけないなと締め切り駆動で動き始めます。発表は2ヶ月後。発表内容はProposal時点で章立てレベルで提出していることもあって大まかに出来上がっていたものの、やはり入門と題して発表するからにはある程度体系立てた知識としてお伝えする必要があるだろうと考えて、分野全体のサーベイを始めました。と言っても、ドキュメント・インテリジェンスという真新しい言葉でサーベイ論文が出ているわけでもなく、ICDARなどの色々な学会の論文やビジネス文書解析がらみのブログ記事を漁ったり、前職の同僚に話を聞いたりと、散らばった情報をかき集めるのはかなり大変でした。特にOCRに関しては、古くからある技術で、ソフトウェアエンジニアに限らず多くの人にとって馴染みある技術だと思うのですが、その技術を日本語で解説した書籍は全くと言っていいいほど見当たりません。見つけた書籍も出版が30年近く前のもので、逆に古典を知る機会になって興味深いということがありました。
最近OCRの歴史について調べていて、1994年出版の文献を見つけて入手できました。電気情報通信学会がまとめた音声や文字認識のパターン認識に関する本で手法自体はあまり書いてないですが、フォントどう対応するかなどの課題が書いてあって面白いです。 pic.twitter.com/RYKCuvIbvo
&amp;mdash; やぐ (@yag_ays) October 6, 2021  こうしたサーベイなどの取り組みの中で今回の資料作成に直接的に寄与した部分は少ないですが、それでも自分の中で分野全体を体系立てて理解するきっかけになったと思います。資料に盛り込めなかった細かな話などは、またどこかブログなどでアウトプットできればと思います。
レシート読み取りという題材 PyCon JPという場所で登壇するからには、あくまでPythonのコードが主役にならなければいけないと考え、チュートリアル的に分析するコードを紹介することにしました。ドキュメント・インテリジェンスは様々なビジネス書類を題材とする都合上、取り組む題材によってやることがかなり変わってきます。なるべく共通した要素を紹介でき、かつ解析の奥深さも伝えられるくらいに難しいもの、そして親しみを持てるトピックを考えた結果、レシートの解析を選択しました。レシートというのはインターネットでも様々な方がブログ記事にしているくらいありふれた題材であり、私が改めて解説するほどのものなのかとかなり悩んだのですが、いざストーリーを構築するなかで簡単な文字列操作から固有表現抽出、暗黙的な表形式などを盛り込むことができ、結果的には私が話したいことを一通り網羅できる良い題材だったと思います。
最後に 今回発表したトピックは自分自身のなかでまだ体系立てて理解していなかった領域だったため、サーベイをしたり説明のためのストーリーを練る作業が本当に大変でした。私にとっては未踏の地の開拓という感じで手探り状態でなんとか完成させた発表ですが、ビジネス書類の解析をふくめ、OCRや周辺領域がもっと活発になり、新しく取り組みたいと思った方にとって多くの情報が得られるようになればいいなと思います。私もサービス開発の中で得られた知見を公開できるよう、これからも頑張りたいと思います。</description>
    </item>
    
    <item>
      <title>MNTSQ &amp; UbieでVertical AI Startup Meetupを開催しました</title>
      <link>https://yag-ays.github.io/article/mntsq-ubie/</link>
      <pubDate>Wed, 11 Aug 2021 09:09:22 +0900</pubDate>
      
      <guid>https://yag-ays.github.io/article/mntsq-ubie/</guid>
      <description>2021/8/10にリーガルテックのMNTSQさんとヘルステックのUbieの2社で&amp;quot;Vertical AI Startup Meetup&amp;quot;というイベントを開催しました。Ubieからは私が登壇したので、当日のイベントの様子や感想をご紹介します。
【MNTSQxUbie】Vertical AI Startup Meetup - connpass
Vertical AI Startupとはなにか @YotaroKatayama まずはじめにMNTSQ 堅山さんから、イベントのタイトルにもなっているVertical AI Startupの紹介と開催 趣旨の説明がありました。このイベントが企画されるまで私はVertical AI Startupという概念を知らなかったのですが、まさにMNTSQさんやUbieのようなスタートアップを表すのにふさわしい単語だと思います。
 リーガルテックやヘルステックといった「Xテック」といった表現で形容されることも多い領域ですが、ただAIや機械学習を使ったピンポイントのソリューションではなく、分野全体をVerticalにカバーする形で課題を解決しにいくんだというところが、Vertical AIという表現のいいところですね（AIという表現に目を瞑れば 笑）。
Vertical AI製品の品質管理 @kzinmr  MNTSQの稲村さんからは製品の品質管理についての発表がありました。少し難しいトピックではありましたが、私自身もぼんやりと考えている概念が簡潔にまとまっており、とても勉強になりました。
教師データを元に学習を行う機械学習においては、機械学習のロジックや特徴量といった要素に加えて、教師データの質も同様に重要です。スライドの中で登場するデータ品質保証というのはまさに予測精度やモデルの質の根幹となる大事な要素ですが、その質を数値として客観的に判断する手段はまだまだ発展途上でかつ、対象ドメインやデータの中身によっても大きく異なります。Uberの事例の引用ではETLのテストで欠損値の割合や重複率といった品質項目が挙げられていましたが、それはデータの質の一つの側面でしかありません。データが生成される背後にある潜在的な分布の変化やデータドリフトをどう捉えるかが重要といった話をされており、MNTSQさんでもかなり苦労されながら品質担保のための項目を充実させているとのことでした。
Ubieの医療においても、入ってくるデータの質が変わるというのは当たり前に起こります。具体のタスクではないですが、例えばインフルエンザは夏に比べて冬の方が多いとか、熱中症は夏によく起きるとか、病気には季節性があります。年単位で見なければデータの傾向を見逃してしまい、夏のデータで何らかの予測モデルを作ったら冬に全然性能が出ないとなってしまうことも考えられます。究極にはデータを見るということが大事という話ですが、人間の事前知識でカバーできない予想外のデータの変化なども含めて、データやそこから作られる機械学習プロダクトの品質に向き合っていく必要がありますね。
医者の言葉、患者の言葉、エンジニアの言葉 @yag_ays  私からはUbieでの実事例を交えながら、医療ドメインにおける言語処理についてお話させていただきました。前回の記事で書いたこの1年の振り返り内容とも少し重複しますが、医療という1つのドメインの中にある複雑さをご紹介できたのではないかと思います。
内容はスライドを見ていただくとして、ここではsli.doで頂いた質問に対して改めてお答えできればと思います。
 医者言葉と患者言葉のマッピングに興味を持ちました。特に患者言葉の曖昧性解消はかなり難しいタスクだと思います（患者自身がどういう現象を表現しているのか理解できずに発した言葉が多そう）が，どういう問題設定でアプローチされているのでしょうか？素朴に文脈を見ても解消できない気がします。
 なかなかガチめの質問で、その場では結構回答するのに苦労しました(笑)。
発表冒頭でも話したように、患者表現の「顔が赤い」を医師は「顔面紅潮」と表現することがあり、同じ対象や現象に対しても話す人によって言い方が異なります。それらを結びつけるためには、エンジニア観点では両者を一旦エンジニアの言葉であるID:42みたいなDB上の一つの項目(エンティティ)に紐付けることになりますが、では自然文からのID変換の名寄せをどうするかが次の課題になってきます。名寄せは前職を含めてここ数年ずっと自分の中で取り組んでいる課題ではあるんですが（昔書いたblog記事）、辞書を拡充させるといった方法以外で解くのは難しい印象です。つまりID:42に対して「顔が赤い」「顔面紅潮」「顔が赤みがかる」「顔が発赤している」みたいな表現をたくさん集めておくという方法ですね。自然言語処理的には単語の分散表現などから意味的な類似度を元に名寄せできると嬉しいのですが、それをするためにはたくさんの学習データを集めて対応関係を学習させるしかなくて、結局やっていることは辞書拡充と同じというのが実情だと思います。近年ではBERTに代表される大規模コーパスでpretrainさせたモデルがどのタスクにも有用な情報を学習できるという話もあり、そこから発展して人間の常識(common knowledge)のようなものが学習できて名寄せにも利用できるといいなと、ぼんやり思っています。
一方、いま現在は曖昧性解消をどうしているかというと、どうもしていないというのが率直な答えになります。今は目の前のタスクをいかに精度高く解くかに注力しており、その上で曖昧性解消を解く必要が出てこない限りは取り組まないです。小枝を切るのにチェーンソーを使う必要がないのと同様に、解きたいタスクに適した最小の技術を当てはめること、それと同時に不必要に難しいタスクを解かないことを意識して取り組んでいます。と、抽象的な話で若干煙に巻いている感はありつつも、実はそろそろ向き合わなければいけなくなってきた雰囲気がするので、今後の進展であったり私が苦労している様子にご期待ください(笑)
We Are Hiring さて、今回はVertical AI Startupという軸でイベントを開催しましたが、MNTSQさんもUbieも機械学習や自然言語処理などの各種エンジニアを募集しております。どちらも採用サイトにてカジュアル面談のフォームがあるので、今回の発表がちょっとでも面白いと思ったら、ぜひ一度お声がけください。お待ちしております！
MNTSQ, Ltd. | リーガルテックカンパニー
Ubie Discovery 採用サイト</description>
    </item>
    
    <item>
      <title>医療ドメインの自然言語処理に飛び込んで1年経って見えてきたこと</title>
      <link>https://yag-ays.github.io/article/mednlp/</link>
      <pubDate>Wed, 23 Jun 2021 09:00:00 +0900</pubDate>
      
      <guid>https://yag-ays.github.io/article/mednlp/</guid>
      <description>医療スタートアップのUbieに入社して1年が経ちました。これまでの人生で一番短かったんじゃないかというくらいのスピードで月日が過ぎ去っていき、主体的に携わるプロジェクトも1.5周くらいしたところかなと思います。この記事では機械学習エンジニアの私が、医療というドメインの自然言語処理に携わるなかで考えたことを紹介したいと思います。
最近ではリーガルテックをはじめ、HR、ファイナンス、そして医療など、様々な領域で自然言語処理の活用が広がっています。そうした専門ドメインでの自然言語処理に携わる人も増えてきていると思いますので、その中の一例として何かしら参考になれば幸いです。
【目次】 - 医療という専門領域の知識は必要 - 分野が違っても手法は同じ、研究が扱う題材を知っておく - 医療という特殊なデータ事情 - なぜ私はいま医療言語処理をやるのか？ - まとめ 医療という専門領域の知識は必要 機械学習で何を解くのかというタスク設計において、医療ドメインの知識は必要不可欠だと感じます。どういった問題やデータに対して、どういった入力/出力を設計し、どうプロダクトに適用するかを、機械学習エンジニアは常日頃考えているわけです。そのあらゆる状況において、医療ドメインの知識は活かされています。
実際に私が遭遇した例を、幾つか紹介しましょう。詳しくは書けませんが、ある病気の症状やその状態を文書中から抜き出すタスクに取り組むことになりました。病気の症状というと「38度の熱がある」とか「昨日から頭が痛い」とか「足首を捻挫した」など、そういったものを想定してください。あまりきっちりとしたタスク定義ができない問題だったのですが、どういう情報を取得できれば価値があるかという問題設定からタスクを検討し始めたなかで見つけたのが、医師が問診の現場で利用する「OPQRST法」というフレームワークでした。これはOnset, Palliative/Provocative, Quality/Quantity, Region/Radiation, Symptoms associated(Severity), Time courseの頭文字を取ったもので、Oはいつから発症したか、Pはどういう時に痛くなる/痛くなくなるかといったように、この枠組みで患者の症状(病歴)を聞くことが良いとされています。これを踏まえて、このタスクではOPQRSTの各項目に対して情報抽出を行うというアプローチを取るということにしました。病気の症状というのは一般の人も知っている概念なので、どうしてもその知識の範囲内だけで捉えてしまいがちですが、医師の目線での体系を使うことでよりタスク定義が明確になったと思います。
また、推論結果のプロダクトへの適用という意味では、既に存在するID体系やオントロジー、リソースといった外部の知識とどう対応付けるかが一つ重要になってきます。つまり、すでに体系立てられているものに寄せて開発したほうがいいよという話です。例えば、文書中から病気の名前を抽出しましょうというタスクがあったときに、その病名の文字列だけを取得できればいいというケースは稀でしょう。その病気の詳細情報を表示したり、インデックスとして利用して検索システムに利用したり、病気に対して何かを紐付けたりと、なんらかダウンストリームのタスクに利用しますが、その時に必要になるのが病気の概念を表したIDです。日本では現在ICD-10と呼ばれる国際疾病分類に従った管理がされており、様々なデータがICD-10準拠になっています。自然言語処理のコーパスにおいても、万病辞書の病気の分類にも採用されています。この他にも様々な医療の概念に対してIDが付与されたり、標準化規格が策定されています。目の前のデータの中だけで精度高く解ければそれでいいということもありますが、既存のID体系やオントロジーに乗っかる方が正確であったり外部知識との接続も容易になることが多く、将来的な汎用性や網羅性に繋がります。こうしたシステムで利用する目的の規格などは医師も知らない場合が多く、エンジニアサイドが主体的に調べて知識を深めていく必要があります。
このように、解くべき問題の背後にある理論や知識は、機械学習タスクを設計する上で重要です。といっても医師と同じような知識が必要というわけではなく、病気や治療に対するモノの捉え方を理解したり、体系化された情報をどう活用するかがポイントと言えます。
分野が違っても手法は同じ、研究が扱う題材を知っておく 専門ドメインでの自然言語処理と言っても、解くべきタスクのデータや性質が異なるだけで、手法としての自然言語処理は共通している場合が多いです。文中からの情報抽出であればCRFやRNN/LSTM、Transformerが使えますし、他にも検索や名寄せ、ポジネガ判定、文書の類似度算出など、広く研究される自然言語処理のタスクの技術をそのまま利用できます。使える道具は共通しているので、そこに関して知識があれば別ドメインに移ることはそう難しいものではありません。
ちなみに分野を俯瞰するという意味では、実際に解かれているタスクを見るのが一番の近道です。私の場合は、はじめにNAIST荒牧先生の「医療言語処理」を読み、あとは適宜タスクが見えてきたら個別にサーベイをして補完していくという流れで進めました。言語処理学会の年次大会の論文から医療に関するものを見つけてきて一通り目を通したり、あとはカンファレンス内で開かれる医療系のワークショップなどは分野に明るくない身としてはとても役に立ちました。
ちなみにUbie入社後ちょっとしてから書いた下記の記事は、一通り実験したあとに類似研究があることに気づいて慌てて言及した記憶があります。何をするにしてもサーベイはやっぱり大事です。
医療分野の大規模テキストデータで学習した分散表現から、疾患の類似度を求める - Out-of-the-box
医療という特殊なデータ事情 ここが医療言語処理の一番の特徴だと思いますが、とにかくデータが無いです。なさすぎて困っています。
無いというのは少し語弊がありますが、もうすこし正確に記述すると、パブリックにアクセスできるコーパスや言語資源に乏しく、病院をはじめ医療関係の企業や研究室といった組織に偏在しているという感じでしょうか。そして色々な自然言語処理のドメインにおいて、医療ほどデータを集めにくい分野は他にないんじゃないかと思います。その理由としては、以下の2つがあります。
 電子カルテや診療記録といった治療行為に関する情報は個人情報の塊であり、容易にアクセスできず、かつ利用が強く制限されるということ 医師や看護師といったドメインエキスパートの労働力は高単価であること  つまり、元となるデータはないわ、作るのにコストがかかるわ、の二重苦なわけです。何かしようと思ってもそもそも機械学習のスタートラインに立てないというのが、毎度おなじみの光景になっています。なので、基本はデータを作るところから始めます。社内DBやウェブサイトなどから使えそうなデータがないか探したり、教師あり学習の場合はアノテーションも付与する必要があるので、こちらもクラウドソーシングや業務委託に依頼するといった形で準備する必要があります。distant supervisionのような半教師有りといった方向に持っていくこともできますが、評価データやベースラインが定まっていない開発初期に採用することはあまりないです。
そうしてデータを作る間にもまだまだやることはたくさんあります。プロダクト開発では、なるべく早い段階で動くモノがあると、依存する他のサービスを作りやすくなります。しかしデータ作成にはリードタイムがありますから、まずはルールベースや簡単なモデルから作り始めます。すると性能を評価をしたくなりますが、評価データもあったりなかったりで、モデルを評価するにも性能向上するにもデータ作成が律速になってしまいがちです。
一方で、医療ドメインでも集めやすいデータが存在するので、そちらを利用する場合はちょっと状況が異なります。例えばTwitterでユーザが発言する病気や症状のテキストであったり、企業の中であれば提供しているウェブサービスが利用される中で得られる各種情報。Googleの研究者らがユーザの検索履歴を元にインフルエンザの流行予測を試みた「Googleインフルトレンド」の話は有名ですね（ちなみに予測はのちに有効ではなかったと結論付けられています link）。他には論文のデータはpublishされているという理由で扱いやすく、英語圏ですと特にPubMedという生命科学に関する論文データベースから、大量の論文のアブストラクト取得することができます。また前節で言及したオントロジーなどの知識ベースも、うまく活用するとよい情報源となります。
このように医療というドメインの性質がデータ収集や利用に強く影響するなかで、なかなか機械学習という枠組みに乗りにくいのが実情です。
なぜ私はいま医療言語処理をやるのか？ 最後に大事な話を一つ。なぜ私はいま医療言語処理に携わるのか？という、もっとも根本的な問いがあります。それは入社時と比べて、より明確になったと思います。
まず第一に社会的な意義。この1年は私にとってUbieの1年であったと同時に新型コロナウィルスの1年でもあったわけですが、医療や健康に対して特に考える機会が多かったと思います。一介の機械学習エンジニアの自分にできることはなんだろうと考えるなかで、医療に対して直接的な貢献はできなくても、医療ベンチャーの一員として手伝えることがあるだろうと改めて思い直しました。世間ではAIや人工知能といった流行で持て囃される業種ですが、真にユーザに価値を届けられる仕組みを作るのはとても大変なことです。そこに対して自然言語処理という領域で取り組むことで、少しでも医療に貢献できればと思っています。
そして第二の理由としては、医療言語処理のプレイヤーの少なさと将来性。特にプロダクト開発/エンジニアリングにおいてプレイヤーは少なく、手のつけられていない課題がたくさんあると思っています。もちろん医療というドメインはその性質上どうしても閉鎖的になりがちな分野で、実際には言語処理関係の仕事をされている方が多数いらっしゃるのは認識しています。そうした制約がある上で分野としての裾野を広げるためにも、共通した言語資源の整備やタスク特化の手法の開発、そして医療言語処理での成功事例の蓄積など、そこに好機を見出して挑戦していくのが楽しいフェーズだと思います。私はこうした知識の共有やアウトリーチ活動が好きなので、個人的な趣向とも一致しています。
まとめ 少し長くなってしまいましたが、この記事では私が医療言語処理という医療ドメインの世界に飛び込んだ中で感じたことを紹介しました。一言で言い表すと、制約だらけの未開の地で大変だけどそのぶん楽しいよ！という感じです。冒頭でも述べたように自然言語処理には医療に限らず色々なドメインが存在しており、医療とは違った形の難しさや挑戦があるのだと思います。そうした自然言語処理がより盛り上がっていくのが楽しみですし、医療言語処理に対しても少しでもそのお手伝いが出来ればと思っております。
さて、Ubieでは、こうした医療言語処理を使ったプロダクトを一緒に開発していく方を募集しております。ご興味がありましたら、ぜひ気軽にご連絡ください。お待ちしております！
シニア機械学習エンジニアの募集 | Ubie株式会社 採用情報
 </description>
    </item>
    
    <item>
      <title>精度向上のために機械学習プロダクト全体をフルスクラッチで書き直した話</title>
      <link>https://yag-ays.github.io/article/refactor-them-all/</link>
      <pubDate>Sun, 24 Jan 2021 14:58:15 +0900</pubDate>
      
      <guid>https://yag-ays.github.io/article/refactor-them-all/</guid>
      <description>2020年7月から医療スタートアップのUbieで機械学習エンジニアをしています。ようやく入社から半年くらいが経ちましたので、ここ最近やっていた仕事として、機械学習プロダクトの精度向上のためにシステム全体をフルスクラッチでかつ一人で実装し直した話をしたいと思います。
機械学習は既に様々な会社でプロダクトに組み込まれ始めていると思いますが、サービスとしてのリリースや長期運用、そして今回お話する継続的な精度向上とリファクタリングについては、公開されている知見はまだまだ少ないと思います。もし同じような境遇の機械学習エンジニアの方への参考になれば幸いです。
tl;dr  精度向上のために、機械学習プロダクト全体をフルスクラッチで書き直した 開発スピードを上げるためには、既存のコードを流用するより新規で書き直すほうが良いような特殊な状況だった 機械学習タスクの実装は、可視化やテストなどを活用しつつ小さく積み上げていくことが大事   はじまり 私が取り組んでいた機械学習プロダクトは、ドキュメントの画像をOCRしてテキスト中から情報を抽出するというサービスでした。画像処理と自然言語処理の領域が入り混じった、なかなかに複雑なタスクです。サービス全体はWebAPIとして実装されており、クラウド上の画像のURLがリクエストとして来ると、画像を読み込んで前処理、OCRに投げて文字を抽出、文字列とその座標位置からいい感じに情報抽出をして、最後に構造化した情報をレスポンスとして返すという構造でした。
今回お話するロジックとは、この中の情報抽出の部分を指します。機械学習プロダクトとしての精度は悪くありませんでしたが、まだまだ精度向上や機能開発の可能性がありました。
精度向上への課題 さて、そのようなプロダクトの精度向上を任されのですが、じゃあKaggleみたいに様々な機械学習モデルを駆使してテストセットの精度を上げていくぜ！となるかというと、そういう話ではありませんでした。よくある機械学習タスクに持ち込めない理由として、具体的には以下のようなものがありました。
 複数の機械学習タスクが存在し、それらに依存関係がある  前段のOCRによる文字検出の結果が、後段の自然言語処理による情報抽出に強く影響する OCRで大きく間違えると、そもそも情報抽出の方ではどうしようもできない   一部のタスクはブラックボックスになっている  OCRは自作しているわけではないため、ブラックボックスとして扱わざるを得ない システム全体で単一のロス関数を設定してそれを下げるみたいなEnd-to-Endなアプローチが難しい   教師データがそもそも少ない  ドメインの性質上、学習データを大量に作ることが容易ではないため、使えるデータが少ない&amp;amp;増やせない 情報抽出の固有表現はバリエーションが非常に多く、全部を網羅するようなデータ集合を作ることは不可能 (分類タスク的に解こうとするとExtreme Multi-label Classificationみたいな設定)    そのため、実際のシステムはディープラーニングでEnd-to-Endに推論するわけではなく、かなり泥臭い方法でロジックを組み立てていき、細かな微調整を繰り返しながら地道に精度を上げていくという感じでした。ディープラーニングが流行る前の、古き良き特徴量エンジニアリング時代の機械学習という感じですね。私が入社した時点ではそういった作業はある程度進んでいる状態でしたので、既存のロジック自体はかなり複雑かつ大きなものになっていました。短期的に精度向上する余地は、未知のパターンに対応したり、辞書を拡充したりといったことくらいでした。
この時点では、見えていた課題に対してこうアプローチすれば精度が改善する！という見立ては立てられていたものの、その手法は既存ロジックを拡張する形では実現できず、わりと根本からのロジックの修正が必要でした。実装の見通しも立っていなかったため、それに手を付けられないまま、細かな修正で少しずつ精度を上げることしかできていませんでした。いわゆる局所最適の谷に嵌っていた感じです。ちなみに他の精度向上施策として一時期はOCR自作も考えましたが、あまりにもRoIのInvestmentが大きいという理由で却下しています。
フルスクラッチで書き換える決意 大きな精度向上のために残された道としては既存ロジックを新しく置き換えるくらいでしたので、ある程度やり尽くした時点でやると決断します。この時に今回の記事の主題である、既存のコードベースはほぼ使わずにシステム全体をフルスクラッチで作ろうと判断しました。
ではなぜ既存のコードベースを活かしつつ該当する箇所だけ置き換えなかったのか？ですが、その理由としては
「開発スピードを上げるため」
これに尽きます。
精度向上を山登りに例えるならば、今登っている登山道を登り続けるよりも、一旦下山して別のルートから頂上にアタックするほうが最終的に頂上に着くのが早いと判断しました。
この時期にちょうど「レガシーコードからの脱却」「リファクタリング」といった本を読んでいたのですが、そこに書かれているアドバイスとしては「レガシーコードのフルスクラッチでの書き換えはやめろ」でした。先人がそうした警鐘を鳴らすなかで、この決定は自分でもかなり葛藤がありました。今でも多くのケースでは、フルスクラッチでの書き換えは悪手だと私は思っています。今回のケースではデメリットを上回るメリットがあると思い、このように決意しました。
開発スピードが落ちる理由 フルスクラッチで書き換える理由は開発スピードを上げるためと書きましたが、その理由としては大きく2つあります。
 画像処理 × 自然言語処理というタスクの難しさ 既存システムの問題  1. 画像処理 × 自然言語処理という難しさ まず何より今回のシステムが、通常のシステム開発や機械学習タスクと比較して特殊だったということがあります。画像をOCRしてテキストから情報抽出するタスクですが、処理の途中でどういう値を扱うかというと、雰囲気はこんな感じです。
# 画像 array([[[243, 245, 244], [242, 244, 243], [241, 243, 242], .</description>
    </item>
    
    <item>
      <title>Sansanを退職してUbieに入社します</title>
      <link>https://yag-ays.github.io/article/ubie/</link>
      <pubDate>Sun, 24 May 2020 23:50:56 +0900</pubDate>
      
      <guid>https://yag-ays.github.io/article/ubie/</guid>
      <description>2018年1月より2年半勤めたSansan株式会社を退社して、2020年7月よりUbie株式会社に入社します。現在は現職の有給消化中で少し気が早いですが、退職エントリを書きたいと思います。前回の転職エントリは手短だったので、今回は自分の仕事内容と絡めつつ互いの会社のことやデータサイエンスのキャリアのことについて長めに書きます。ちなみに、私のプロフィールは https://yag.xyz/ をご覧ください。
これまでの仕事 Sansanは法人/個人向けのクラウド名刺管理を提供している会社で、名刺のデータ化に関わる開発やデータ活用を推進しているDSOC (Data Strategy &amp;amp; Operation Center)という部署に入りました。Sansanに中途入社する前に在籍していたリクルートテクノロジーズでは機械学習のプロダクト開発を担当しており、Sansanでも引き続き機械学習でサービスに貢献するような技術開発を行っていました。特に、自然言語処理に関する機械学習に携わらせていただきました。
名刺管理のSaaSという特殊なサービスの性質上、自然言語処理や機械学習のタスクにおいても他社ではあまり取り組まれない特殊なタスクが多かったと思います。下記のような仕事をしていました。
 名刺とニュースを紐付ける - 深層学習を用いた記事文章からの企業名抽出 名刺を超えて人や企業を検索する - Sansanにおける検索システムへの取り組み 文字のゆらぎをどう扱うか？ - Sansanにおける自然言語処理の活用  これらの中でも特に惹かれたのは「名寄せ」と「情報抽出」です。具体的なタスクとしては、様々な表記ゆれがある単語を正規化するものであったり、文章中から企業名や日付など特定の概念(エンティティ)を抽出するものだったりするのですが、少量のデータしかなかったり、一般常識や固有の知識が必要だったり、正解が厳密に定義できないようなタスクなど、ビジネスの現場で遭遇するものは特に難しいと感じます。
Sansanの中でこうした困難なタスクにどっしりと向き合えたのは、とても貴重な経験でした。一方で、ビジネス上高い精度が求められるがために機械学習の機能を導入できなかったりと、周囲の期待に答えられないもどかしさもありました。こうした経験を通じて、これらのテーマは私の中で人生をかけて向き合うタスクになったと思います。
また、アカデミックとの接点では、Wikipedia構造化プロジェクトである森羅に参加させてもらったり、NLPやIBISにポスターを出したり、DEIMやCCSEで講演する機会もいただきました。部署的にも対外的な発信を推奨されている文化であり、かつ社内チェックなどの体制もしっかりしているので、安心して発表しつつ資料をパブリックに公開することができました。
あと、気がつけば新卒/中途採用を任されたり、長期インターンのメンターを受け持ったりしました。2年半しかいなかったのですが、チームの中で中堅社員くらいの立ち位置&amp;amp;役割だったと思います。
Sansan DSOCの環境 R&amp;amp;Dチームとして SansanのR&amp;amp;Dには数多くの優秀なメンバーがいて刺激になりました。この規模の会社で、画像処理から機械学習、自然言語処理、そして社会科学に至る多様なメンバーが在籍している組織は、周りを見渡してもそうそう無いのではないかと思います。博士号を持つ人間も多く、社会人博士に通いながら仕事をしている人がいるのもレベルの高さを感じさせます。他にわかりやすい例としては、Kaggle Grandmasterの方だったり、東大の助教からSansanに入社した方もいらっしゃいます。
特に、同僚で一緒に自然言語処理のタスクに取り組んだ@kanji250trは、エンジニアリングも出来て研究もできるというフルスタック人材でした。自然言語処理タスクに関して議論したり、一緒にプロダクトを作ったり、NLPにポスターを出したりと、彼と一緒に仕事が出来て本当に良かったと思います。
このR&amp;amp;D組織が特にユニークだと思うのは、社会科学系の方々がめざましい活躍をしているところです。名刺交換の先にある「人と人との出会い」というデータに対して、科学的アプローチで構造を理解したり、サービスの価値向上のための分析を行ったりと、Sansanにしかないデータで研究し価値を出しているのは素晴らしいと思います。コンピューターサイエンス側からするとあまり想像がつかないと思いますので、私が面白いと感じた活動を貼っておきます。
 Sansanの名刺ネットワーク分析からみえてきた「成功するビジネスの出会い」の要件 | BizHint（ビズヒント）- 事業の課題にヒントを届けるビジネスメディア 「SocSci Meetup～社会科学をブートする～」イベントレポート - Sansan Builders Box  また、私が入社したときには中途採用の人間の割合が多かったのですが、ここ数年で優秀な新卒も続々入社してきており、年齢的にも若い組織になってきていると思います。
DSOC全体として 所属していた部署全体としても恵まれた環境でした。DSOCの中には、先に紹介したR&amp;amp;Dチームの他にも、開発やインフラ、運用などサービス全体に関わるチームが所属しています。そのため、データサイエンティストは分析だけするみたいな閉じた状態にならず、様々な方と接する中で仕事のバランスが取れたと思います。
機械学習エンジニアとして一番ありがたかったのは、社内でデータのアノテーションが完了するところでしょうか。Sansanは名刺をデータ化する上でクラウドソーシング先を多く抱えており、DSOCにはデータ化体制を運用するチームがいます。機械学習タスクの学習データを大規模に作れる体制が整っているということです。以前そのチームと仕事をしていたときは、学習データを用意する必要がありますねと言った瞬間にはそれ作ろうという話が進み、数日するとアノテーションガイドラインと対象データの準備が完了し、数週間する頃には学習データが完成していました。機械学習エンジニア側はデータセットの内容にだけ注力し、その他の発注作業や取りまとめ、クオリティコントロール等は専属チームに任せられたので、本当に仕事がやりやすかったです。
まだまだ紹介したいのですが最後に一つだけ。私がいた部署にはクリエイティブグループという部署全体のブランディングやデザインを担当するチームがあり、私が作ったポンチ絵とゴミスライドを投げると、統一感あるハイクオリティな資料に仕上げてくれました。これは思った以上に感動的な体験だったので、いろんな会社で普及するといいんじゃないかと真面目に思います。
福利厚生 あとはこんな福利厚生が良かったです。
 フルスペックMacBook Pro &amp;amp; 昇降机 &amp;amp; 曲面ディスプレイ  PCはWindows/macOS、ディスプレイは複数/42inch/曲面など選択できます   本や学習に年6万円、ハードウェアに年3万円、ソフトウェアに年2万円の支給  書籍は躊躇なく買えましたし、AirPods Proやリモートワーク用の良いマイクを買いました   土日出勤して休みを平日に振り替える制度  振替休日と土日と繋げて、国内/海外旅行に行けました   リファラル採用に関する諸々の支援  紹介する側もされる側もインセンティブがあります    転職活動 このような業務や環境でわりかし自由に仕事させてもらっていたのですが、自身の成長を客観視したり、界隈の機械学習人材の凄さみたいなものを知れば知るほど、今の自分はコンフォートゾーンに入ってしまいっているのではという不安が常にありました。つい2,3年前に最先端だった技術が今では新しいものに置き換わってしまうこの分野においては、安定すればするほど不安になるというのは仕方がないことかもしれません。</description>
    </item>
    
  </channel>
</rss>
