<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Article on Out-of-the-box</title>
    <link>https://yag-ays.github.io/article/</link>
    <description>Recent content in Article on Out-of-the-box</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja</language>
    <lastBuildDate>Sun, 24 Jan 2021 14:58:15 +0900</lastBuildDate><atom:link href="https://yag-ays.github.io/article/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>精度向上のために機械学習プロダクト全体をフルスクラッチで書き直した話</title>
      <link>https://yag-ays.github.io/article/refactor-them-all/</link>
      <pubDate>Sun, 24 Jan 2021 14:58:15 +0900</pubDate>
      
      <guid>https://yag-ays.github.io/article/refactor-them-all/</guid>
      <description>2020年7月から医療スタートアップのUbieで機械学習エンジニアをしています。ようやく入社から半年くらいが経ちましたので、ここ最近やっていた仕事として、機械学習プロダクトの精度向上のためにシステム全体をフルスクラッチでかつ一人で実装し直した話をしたいと思います。
機械学習は既に様々な会社でプロダクトに組み込まれ始めていると思いますが、サービスとしてのリリースや長期運用、そして今回お話する継続的な精度向上とリファクタリングについては、公開されている知見はまだまだ少ないと思います。もし同じような境遇の機械学習エンジニアの方への参考になれば幸いです。
tl;dr  精度向上のために、機械学習プロダクト全体をフルスクラッチで書き直した 開発スピードを上げるためには、既存のコードを流用するより新規で書き直すほうが良いような特殊な状況だった 機械学習タスクの実装は、可視化やテストなどを活用しつつ小さく積み上げていくことが大事   はじまり 私が取り組んでいた機械学習プロダクトは、ドキュメントの画像をOCRしてテキスト中から情報を抽出するというサービスでした。画像処理と自然言語処理の領域が入り混じった、なかなかに複雑なタスクです。サービス全体はWebAPIとして実装されており、クラウド上の画像のURLがリクエストとして来ると、画像を読み込んで前処理、OCRに投げて文字を抽出、文字列とその座標位置からいい感じに情報抽出をして、最後に構造化した情報をレスポンスとして返すという構造でした。
今回お話するロジックとは、この中の情報抽出の部分を指します。機械学習プロダクトとしての精度は悪くありませんでしたが、まだまだ精度向上や機能開発の可能性がありました。
精度向上への課題 さて、そのようなプロダクトの精度向上を任されのですが、じゃあKaggleみたいに様々な機械学習モデルを駆使してテストセットの精度を上げていくぜ！となるかというと、そういう話ではありませんでした。よくある機械学習タスクに持ち込めない理由として、具体的には以下のようなものがありました。
 複数の機械学習タスクが存在し、それらに依存関係がある  前段のOCRによる文字検出の結果が、後段の自然言語処理による情報抽出に強く影響する OCRで大きく間違えると、そもそも情報抽出の方ではどうしようもできない   一部のタスクはブラックボックスになっている  OCRは自作しているわけではないため、ブラックボックスとして扱わざるを得ない システム全体で単一のロス関数を設定してそれを下げるみたいなEnd-to-Endなアプローチが難しい   教師データがそもそも少ない  ドメインの性質上、学習データを大量に作ることが容易ではないため、使えるデータが少ない&amp;amp;増やせない 情報抽出の固有表現はバリエーションが非常に多く、全部を網羅するようなデータ集合を作ることは不可能 (分類タスク的に解こうとするとExtreme Multi-label Classificationみたいな設定)    そのため、実際のシステムはディープラーニングでEnd-to-Endに推論するわけではなく、かなり泥臭い方法でロジックを組み立てていき、細かな微調整を繰り返しながら地道に精度を上げていくという感じでした。ディープラーニングが流行る前の、古き良き特徴量エンジニアリング時代の機械学習という感じですね。私が入社した時点ではそういった作業はある程度進んでいる状態でしたので、既存のロジック自体はかなり複雑かつ大きなものになっていました。短期的に精度向上する余地は、未知のパターンに対応したり、辞書を拡充したりといったことくらいでした。
この時点では、見えていた課題に対してこうアプローチすれば精度が改善する！という見立ては立てられていたものの、その手法は既存ロジックを拡張する形では実現できず、わりと根本からのロジックの修正が必要でした。実装の見通しも立っていなかったため、それに手を付けられないまま、細かな修正で少しずつ精度を上げることしかできていませんでした。いわゆる局所最適の谷に嵌っていた感じです。ちなみに他の精度向上施策として一時期はOCR自作も考えましたが、あまりにもRoIのInvestmentが大きいという理由で却下しています。
フルスクラッチで書き換える決意 大きな精度向上のために残された道としては既存ロジックを新しく置き換えるくらいでしたので、ある程度やり尽くした時点でやると決断します。この時に今回の記事の主題である、既存のコードベースはほぼ使わずにシステム全体をフルスクラッチで作ろうと判断しました。
ではなぜ既存のコードベースを活かしつつ該当する箇所だけ置き換えなかったのか？ですが、その理由としては
「開発スピードを上げるため」
これに尽きます。
精度向上を山登りに例えるならば、今登っている登山道を登り続けるよりも、一旦下山して別のルートから頂上にアタックするほうが最終的に頂上に着くのが早いと判断しました。
この時期にちょうど「レガシーコードからの脱却」「リファクタリング」といった本を読んでいたのですが、そこに書かれているアドバイスとしては「レガシーコードのフルスクラッチでの書き換えはやめろ」でした。先人がそうした警鐘を鳴らすなかで、この決定は自分でもかなり葛藤がありました。今でも多くのケースでは、フルスクラッチでの書き換えは悪手だと私は思っています。今回のケースではデメリットを上回るメリットがあると思い、このように決意しました。
開発スピードが落ちる理由 フルスクラッチで書き換える理由は開発スピードを上げるためと書きましたが、その理由としては大きく2つあります。
 画像処理 × 自然言語処理というタスクの難しさ 既存システムの問題  1. 画像処理 × 自然言語処理という難しさ まず何より今回のシステムが、通常のシステム開発や機械学習タスクと比較して特殊だったということがあります。画像をOCRしてテキストから情報抽出するタスクですが、処理の途中でどういう値を扱うかというと、雰囲気はこんな感じです。
# 画像 array([[[243, 245, 244], [242, 244, 243], [241, 243, 242], .</description>
    </item>
    
    <item>
      <title>Sansanを退職してUbieに入社します</title>
      <link>https://yag-ays.github.io/article/ubie/</link>
      <pubDate>Sun, 24 May 2020 23:50:56 +0900</pubDate>
      
      <guid>https://yag-ays.github.io/article/ubie/</guid>
      <description>2018年1月より2年半勤めたSansan株式会社を退社して、2020年7月よりUbie株式会社に入社します。現在は現職の有給消化中で少し気が早いですが、退職エントリを書きたいと思います。前回の転職エントリは手短だったので、今回は自分の仕事内容と絡めつつ互いの会社のことやデータサイエンスのキャリアのことについて長めに書きます。ちなみに、私のプロフィールは https://yag.xyz/ をご覧ください。
これまでの仕事 Sansanは法人/個人向けのクラウド名刺管理を提供している会社で、名刺のデータ化に関わる開発やデータ活用を推進しているDSOC (Data Strategy &amp;amp; Operation Center)という部署に入りました。Sansanに中途入社する前に在籍していたリクルートテクノロジーズでは機械学習のプロダクト開発を担当しており、Sansanでも引き続き機械学習でサービスに貢献するような技術開発を行っていました。特に、自然言語処理に関する機械学習に携わらせていただきました。
名刺管理のSaaSという特殊なサービスの性質上、自然言語処理や機械学習のタスクにおいても他社ではあまり取り組まれない特殊なタスクが多かったと思います。下記のような仕事をしていました。
 名刺とニュースを紐付ける - 深層学習を用いた記事文章からの企業名抽出 名刺を超えて人や企業を検索する - Sansanにおける検索システムへの取り組み 文字のゆらぎをどう扱うか？ - Sansanにおける自然言語処理の活用  これらの中でも特に惹かれたのは「名寄せ」と「情報抽出」です。具体的なタスクとしては、様々な表記ゆれがある単語を正規化するものであったり、文章中から企業名や日付など特定の概念(エンティティ)を抽出するものだったりするのですが、少量のデータしかなかったり、一般常識や固有の知識が必要だったり、正解が厳密に定義できないようなタスクなど、ビジネスの現場で遭遇するものは特に難しいと感じます。
Sansanの中でこうした困難なタスクにどっしりと向き合えたのは、とても貴重な経験でした。一方で、ビジネス上高い精度が求められるがために機械学習の機能を導入できなかったりと、周囲の期待に答えられないもどかしさもありました。こうした経験を通じて、これらのテーマは私の中で人生をかけて向き合うタスクになったと思います。
また、アカデミックとの接点では、Wikipedia構造化プロジェクトである森羅に参加させてもらったり、NLPやIBISにポスターを出したり、DEIMやCCSEで講演する機会もいただきました。部署的にも対外的な発信を推奨されている文化であり、かつ社内チェックなどの体制もしっかりしているので、安心して発表しつつ資料をパブリックに公開することができました。
あと、気がつけば新卒/中途採用を任されたり、長期インターンのメンターを受け持ったりしました。2年半しかいなかったのですが、チームの中で中堅社員くらいの立ち位置&amp;amp;役割だったと思います。
Sansan DSOCの環境 R&amp;amp;Dチームとして SansanのR&amp;amp;Dには数多くの優秀なメンバーがいて刺激になりました。この規模の会社で、画像処理から機械学習、自然言語処理、そして社会科学に至る多様なメンバーが在籍している組織は、周りを見渡してもそうそう無いのではないかと思います。博士号を持つ人間も多く、社会人博士に通いながら仕事をしている人がいるのもレベルの高さを感じさせます。他にわかりやすい例としては、Kaggle Grandmasterの方だったり、東大の助教からSansanに入社した方もいらっしゃいます。
特に、同僚で一緒に自然言語処理のタスクに取り組んだ@kanji250trは、エンジニアリングも出来て研究もできるというフルスタック人材でした。自然言語処理タスクに関して議論したり、一緒にプロダクトを作ったり、NLPにポスターを出したりと、彼と一緒に仕事が出来て本当に良かったと思います。
このR&amp;amp;D組織が特にユニークだと思うのは、社会科学系の方々がめざましい活躍をしているところです。名刺交換の先にある「人と人との出会い」というデータに対して、科学的アプローチで構造を理解したり、サービスの価値向上のための分析を行ったりと、Sansanにしかないデータで研究し価値を出しているのは素晴らしいと思います。コンピューターサイエンス側からするとあまり想像がつかないと思いますので、私が面白いと感じた活動を貼っておきます。
 Sansanの名刺ネットワーク分析からみえてきた「成功するビジネスの出会い」の要件 | BizHint（ビズヒント）- 事業の課題にヒントを届けるビジネスメディア 「SocSci Meetup～社会科学をブートする～」イベントレポート - Sansan Builders Box  また、私が入社したときには中途採用の人間の割合が多かったのですが、ここ数年で優秀な新卒も続々入社してきており、年齢的にも若い組織になってきていると思います。
DSOC全体として 所属していた部署全体としても恵まれた環境でした。DSOCの中には、先に紹介したR&amp;amp;Dチームの他にも、開発やインフラ、運用などサービス全体に関わるチームが所属しています。そのため、データサイエンティストは分析だけするみたいな閉じた状態にならず、様々な方と接する中で仕事のバランスが取れたと思います。
機械学習エンジニアとして一番ありがたかったのは、社内でデータのアノテーションが完了するところでしょうか。Sansanは名刺をデータ化する上でクラウドソーシング先を多く抱えており、DSOCにはデータ化体制を運用するチームがいます。機械学習タスクの学習データを大規模に作れる体制が整っているということです。以前そのチームと仕事をしていたときは、学習データを用意する必要がありますねと言った瞬間にはそれ作ろうという話が進み、数日するとアノテーションガイドラインと対象データの準備が完了し、数週間する頃には学習データが完成していました。機械学習エンジニア側はデータセットの内容にだけ注力し、その他の発注作業や取りまとめ、クオリティコントロール等は専属チームに任せられたので、本当に仕事がやりやすかったです。
まだまだ紹介したいのですが最後に一つだけ。私がいた部署にはクリエイティブグループという部署全体のブランディングやデザインを担当するチームがあり、私が作ったポンチ絵とゴミスライドを投げると、統一感あるハイクオリティな資料に仕上げてくれました。これは思った以上に感動的な体験だったので、いろんな会社で普及するといいんじゃないかと真面目に思います。
福利厚生 あとはこんな福利厚生が良かったです。
 フルスペックMacBook Pro &amp;amp; 昇降机 &amp;amp; 曲面ディスプレイ  PCはWindows/macOS、ディスプレイは複数/42inch/曲面など選択できます   本や学習に年6万円、ハードウェアに年3万円、ソフトウェアに年2万円の支給  書籍は躊躇なく買えましたし、AirPods Proやリモートワーク用の良いマイクを買いました   土日出勤して休みを平日に振り替える制度  振替休日と土日と繋げて、国内/海外旅行に行けました   リファラル採用に関する諸々の支援  紹介する側もされる側もインセンティブがあります    転職活動 このような業務や環境でわりかし自由に仕事させてもらっていたのですが、自身の成長を客観視したり、界隈の機械学習人材の凄さみたいなものを知れば知るほど、今の自分はコンフォートゾーンに入ってしまいっているのではという不安が常にありました。つい2,3年前に最先端だった技術が今では新しいものに置き換わってしまうこの分野においては、安定すればするほど不安になるというのは仕方がないことかもしれません。</description>
    </item>
    
  </channel>
</rss>
