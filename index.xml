<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Out-of-the-box</title>
    <link>https://yag-ays.github.io/</link>
    <description>Recent content on Out-of-the-box</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-US</language>
    <lastBuildDate>Thu, 23 Aug 2018 07:41:44 +0900</lastBuildDate>
    
	<atom:link href="https://yag-ays.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>📙Unicode絵文字の日本語読み/キーワード/分類辞書📙</title>
      <link>https://yag-ays.github.io/project/emoji-ja/</link>
      <pubDate>Thu, 23 Aug 2018 07:41:44 +0900</pubDate>
      
      <guid>https://yag-ays.github.io/project/emoji-ja/</guid>
      <description>emoji_jaは、Unicodeに登録されている絵文字に対して、日本語の読みやキーワード、分類を付与したデータセットです。Unicodeで定められている名称やアノテーションを元に構築しています。
TwitterやInstagramなどのSNSを通じた絵文字の普及により、emoji2vecやdeepmojiなどの絵文字を使った自然言語処理の研究が行われるようになりました。絵文字を含む分析においては、絵文字の持つ豊富な情報や多彩な利用方法により、従来の形態素分析などのテキスト処理では対応できない場合があります。例えば、「今日は楽しかった😀」という文章では感情表現として絵文字が使われていますが、「今日は🍣を食べて🍺を飲んだ」ではそれぞれの対象を表す単語として用いられることもあります。[佐藤,2015]では絵文字の品詞を名詞/サ変名詞/動詞/副詞/記号/感動詞の6種類に分類しており、形態素解析に用いるNEologd辞書にも絵文字が登録されています。
このように、絵文字を機械的な処理や研究対象として扱うには、絵文字の読み方であったり意味を表す単語、または意味的な種類で分類したカテゴリが必要になります。こうした辞書は、英語においてはemojilibがありますが、絵文字は文化的に意味が異なる場合があるため、それらの対訳をそのまま利用できないことがあります。
そのため、日本語で容易に使えるリソースとしてemoji_jaを作成しました。
💻 ダウンロード 以下のGitHubレポジトリからjson形式のファイルをダウンロードできます。data/配下にある各種jsonファイルが、データセットの本体です。
yagays/emoji-ja: 📙UNICODE絵文字の日本語読み/キーワード/分類辞書📙
📁 データセット emoji-jaには下記の3種類のデータが含まれています。
 emoji_ja.json: 絵文字に対応するキーワードやメタ情報 group2emoji_ja.json: 絵文字のグループ/サブグループに対応した絵文字のリスト keyword2emoji_ja.json: 絵文字のキーワードに対応した絵文字のリスト  1️⃣ emoji_ja.jsonデータ emoji_ja.jsonには、絵文字に対応する以下のメタデータが含まれています。
   カラム 概要 取得元     keywords 絵文字に関連したキーワード CJK Annotations (CLDR Version 33)   short_name 絵文字を表す短い名前 CJK Annotations (CLDR Version 33)   group 絵文字を意味的に分類したときのグループ Emoji List, v11.0を元に翻訳   subgroup 絵文字を意味的に分類したときのサブグループ Emoji List, v11.0を元に翻訳    { &amp;quot;♟&amp;quot;: { &amp;quot;keywords&amp;quot;: [ &amp;quot;チェス&amp;quot;, &amp;quot;チェスの駒&amp;quot;, &amp;quot;捨て駒&amp;quot;, &amp;quot;駒&amp;quot; ], &amp;quot;short_name&amp;quot;: &amp;quot;チェスの駒&amp;quot;, &amp;quot;group&amp;quot;: &amp;quot;活動&amp;quot;, &amp;quot;subgroup&amp;quot;: &amp;quot;ゲーム&amp;quot; }, &amp;quot;♾&amp;quot;: { &amp;quot;keywords&amp;quot;: [ &amp;quot;万物&amp;quot;, &amp;quot;永遠&amp;quot;, &amp;quot;無限&amp;quot;, &amp;quot;無限大&amp;quot; ], &amp;quot;short_name&amp;quot;: &amp;quot;無限大&amp;quot;, &amp;quot;group&amp;quot;: &amp;quot;記号&amp;quot;, &amp;quot;subgroup&amp;quot;: &amp;quot;その他 シンボル&amp;quot; }, .</description>
    </item>
    
    <item>
      <title>漢字を構成する部首/偏旁のデータセット</title>
      <link>https://yag-ays.github.io/project/kanjivg-radical/</link>
      <pubDate>Mon, 06 Aug 2018 08:43:23 +0900</pubDate>
      
      <guid>https://yag-ays.github.io/project/kanjivg-radical/</guid>
      <description>kanjivg-radicalは、漢字を構成する部首や偏旁を容易に扱えるように対応付けしたデータセットです。
「脳」という漢字は、「月」「⺍」「凶」のように幾つかのまとまりごとに細分化できます。このように意味ある要素に分解しデータセットにすることで、漢字を文字的に分解して扱ったり、逆に特定の部首/偏旁を持つ漢字を一括して検索することができます。
このデータセットは、KanjiVGで公開されているsvgデータを抽出および加工して作成されています。そのため、本データセットに含まれる部首/偏旁のアノテーションはすべてKanjiVGに準拠します。
ダウンロード 以下のGitHubレポジトリからjson形式のファイルをダウンロードできます。data/配下にある各種jsonファイルが、データセットの本体です。
yagays/kanjivg-radical
データセットの詳細 kanjivg-radicalには4種類のデータが含まれています。
 漢字と部首/偏旁の変換データ 漢字と要素の変換データ 漢字と部首/偏旁の変換データのうち、左右に分割できる漢字 漢字と部首/偏旁の変換データのうち、上下に分割できる漢字  以下では、部首/偏旁はradical、要素はelement、左右はleft_right、上下はtop_buttomと表現しています。
1. 漢字と部首/偏旁の変換データ 漢字と部首/偏旁を対応付けしたデータセットです。漢字から部首/偏旁と、部首/偏旁から漢字の2種類のデータがあります。
 kanji2radical.json : 漢字から部首/偏旁への変換 radical2kanji.json : 部首/偏旁から漢字への変換  # kanji2radical.jsonのサンプル &amp;quot;脳&amp;quot;: [ &amp;quot;月&amp;quot;, &amp;quot;⺍&amp;quot;, &amp;quot;凶&amp;quot; ]  # radical2kanji.jsonのサンプル &amp;quot;月&amp;quot;: [ &amp;quot;肝&amp;quot;, &amp;quot;育&amp;quot;, &amp;quot;胆&amp;quot;, &amp;quot;朦&amp;quot;, &amp;quot;脱&amp;quot;, ...  2. 漢字と要素の変換データ 漢字と要素を対応付けしたデータセットです。漢字から要素と、要素から漢字の2種類のデータがあります。
 kanji2element.json : 漢字から要素への変換 element2kanji.json : 要素から漢字への変換  ここで使用している「要素」いう言葉は、部首/偏旁を構成する更に細かい単位での漢字のことを指しています。言語学的に定義された単語ではなく、KanjiVGで利用されていたelementの対訳として用いています。
このデータでは構成している要素をすべて列挙しているので、結果の中には重複が含まれます。以下の例だと脳には「凶」という要素が含まれていますが、同時に「乂」という要素も含まれているため、どちらもkanji2elementの結果として出力されます。
# kanji2element.jsonのサンプル &amp;quot;脳&amp;quot;: [ &amp;quot;乂&amp;quot;, &amp;quot;凶&amp;quot;, &amp;quot;丿&amp;quot;, &amp;quot;凵&amp;quot;, &amp;quot;⺍&amp;quot;, &amp;quot;月&amp;quot; ]  # element2kanji.</description>
    </item>
    
    <item>
      <title>Wikipedia CirrusSearchのダンプデータを利用する</title>
      <link>https://yag-ays.github.io/project/cirrus/</link>
      <pubDate>Mon, 30 Jul 2018 21:07:52 +0900</pubDate>
      
      <guid>https://yag-ays.github.io/project/cirrus/</guid>
      <description>Wikipediaのデータを容易に利用できるCirrusSearchのダンプデータについて紹介します。これを利用することにより、Wikipediaの巨大なXMLデータをパースしたり、Wikipedia Extractorなど既存のツールで前処理することなく、直にWikipediaの各種データにアクセスすることができます。
tl;dr 細かいことは置いておいて、素直にWikipediaの日本語エントリーに書かれているテキストを取得したい場合、
 ここのCirrusSearchの任意の日付のダンプデータjawiki-YYYYMMDD-cirrussearch-content.json.gzを落としてくる 中に入っているjsonデータをパースして、偶数行の&amp;quot;text&amp;quot;を取得するコードを書く  とすることで、簡単にWikipediaのテキストデータを取得することができます。
CirrusSearchダンプデータの概要 CirrusSearchは、ElasticSearchをバックエンドに構成された検索システムです。このシステムに利用されているデータがダンプデータとして公開されており、そのファイルを利用することで、整形されたテキストを始めとして、外部リンクのリストやカテゴリのリスト等のメタデータが容易に利用できます。また、言語ごとにダンプファイルが分かれているため、日本語のWikipediaのデータだけを対象にすることが可能です。
CirrusSearchのダンプデータは以下から取得します。
Index of /other/cirrussearch/
Wikipediaに関するダンプデータは以下の2つです。
   ファイル 内容     jawiki-YYYYMMDD-cirrussearch-content.json.gz Wikipediaの本文（namespaceが0）   jawiki-YYYYMMDD-cirrussearch-general.json.gz Wikipediaのその他情報（namespaceが0以外）    その他の接頭辞の対応関係は以下の通りです。
 jawiki: ウィキペディア jawikibooks: ウィキブックス jawikinews: ウィキニュース jawikiquote: ウィキクォート jawikisource: ウィキソース jawikiversity: ウィキバーシティ  CirrusSearchのデータ CirrusSearchのダンプデータは、1行が1つのjsonとなっており、2行で1つのエントリーを表しています。
奇数行 奇数行にはエントリーに固有のidが記載されています。この_idから該当のエントリーにアクセスするには、https://ja.wikipedia.org/?curid=3240437のようにcuridのパラーメータを指定します。
{ &amp;quot;index&amp;quot;: { &amp;quot;_type&amp;quot;: &amp;quot;page&amp;quot;, &amp;quot;_id&amp;quot;: &amp;quot;3240437&amp;quot; } }  偶数行 偶数行にはエントリーの情報が記載されています。下記の例では、複数の要素が入った配列や長い文字列は...で省略しています。
{ &amp;quot;template&amp;quot;: [ &amp;quot;Template:各年の文学ヘッダ&amp;quot;, .</description>
    </item>
    
    <item>
      <title>Home</title>
      <link>https://yag-ays.github.io/home/</link>
      <pubDate>Thu, 26 Jul 2018 04:28:29 +0900</pubDate>
      
      <guid>https://yag-ays.github.io/home/</guid>
      <description>yag_aysの資材置き場。out-of-the-boxなデータセット/コーパス/ノウハウを公開していきたい。
記事一覧はこちら→ PROJECT</description>
    </item>
    
    <item>
      <title>文字の図形的な埋め込み表現 Glyph-aware Character Embedding</title>
      <link>https://yag-ays.github.io/project/char-embedding/</link>
      <pubDate>Wed, 25 Jul 2018 12:30:41 +0900</pubDate>
      
      <guid>https://yag-ays.github.io/project/char-embedding/</guid>
      <description>「文字の図形的な埋め込み表現」は、文字の図形的な情報から埋め込み表現を学習したデータセットです。文字の意味や文章中の文脈などのセマンティクスから構成する分散表現とは違い、文字の形状という視覚的な特徴を学習しています。それぞれの文字に対する埋め込み表現の近さを計算することで、似た形の文字を推定することができます。
ダウンロード 下記のGitHubレポジトリからダウンロード可能です。以下のURLを開いて「Download」をクリックしてください。
convolutional_AE_300.tar.bz2 (解凍前:88MB, 解凍後:180MB)
以下の2つのファイルが入っています。フォーマットが異なるだけで、どちらも同じベクトルデータです。
 convolutional_AE_300.bin convolutional_AE_300.txt  その他サンプルコードなどのすべてのファイルは、以下のレポジトリにあります。
yagays/glyph-aware-character-embedding
詳細  ベクトル次元：300 文字の種類数：44,637 学習データに用いたフォント：Google Noto Fonts NotoSansCJKjp-Regular  使い方 gensimを用いた利用方法を例示します。なお、ここではword2vecのように単語の分散表現として扱っていますが、本リソースで扱う文字の図形的な埋め込み表現には加法性がありません。図形としての文字の類似度は計算できますが、部首の足し算引き算といったような操作はできないのでご注意下さい。
from gensim.models import KeyedVectors model = KeyedVectors.load_word2vec_format(&amp;quot;data/convolutional_AE_300.bin&amp;quot;, binary=True)  most_similar()を用いて、図形的な類似文字を検索します。以下の例では一番類似度が高い文字に「а」が来ていますが、これはasciiの「a」ではなくキリル文字の「a」です。
In []: model.most_similar(&amp;quot;a&amp;quot;) Out[]: [(&#39;а&#39;, 1.0000001192092896), (&#39;ả&#39;, 0.961397111415863), (&#39;ä&#39;, 0.9610118269920349), (&#39;ā&#39;, 0.9582812190055847), (&#39;á&#39;, 0.957198441028595), (&#39;à&#39;, 0.9558833241462708), (&#39;å&#39;, 0.938391923904419), (&#39;ầ&#39;, 0.9370290040969849), (&#39;ǎ&#39;, 0.9368112087249756), (&#39;ấ&#39;, 0.9365179538726807)]  Google Noto Fonts NotoSansCJKjp-Regularに含まれるすべての文字に対して操作が可能です。
In []: model.most_similar(&amp;quot;油&amp;quot;) Out[]: [(&#39;汕&#39;, 0.9025427103042603), (&#39;泊&#39;, 0.</description>
    </item>
    
  </channel>
</rss>